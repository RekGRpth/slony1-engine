<article id="slonstart"> <title/Slon daemons/

<para>The programs that actually perform Slony-I replication are the "slon" daemons.

<para>You need to run one "slon" instance for each node in a Slony-I cluster, whether you consider that node a "master" or a "slave."  Since a MOVE SET or FAILOVER can switch the roles of nodes, slon needs to be able to function for both providers and subscribers.  It is not essential that these daemons run on any particular host, but there are some principles worth considering:
<itemizedlist>
<listitem><Para> Each slon needs to be able to communicate quickly with the database whose "node controller" it is.  Therefore, if a Slony-I cluster runs across some form of Wide Area Network, each slon process should run on or nearby the databases each is controlling.  If you break this rule, no particular disaster should ensue, but the added latency introduced to monitoring events on the slon's "own node" will cause it to replicate in a _somewhat_ less timely manner.

<listitem><Para> The fastest results would be achieved by having each slon run on the database server that it is servicing.  If it runs somewhere within a fast local network, performance will not be noticeably degraded.

<listitem><Para> It is an attractive idea to run many of the slon processes for a cluster on one machine, as this makes it easy to monitor them both in terms of log files and process tables from one location.  This eliminates the need to login to several hosts in order to look at log files or to restart slon instances.
</itemizedlist>

<para>There are two "watchdog" scripts currently available:
<itemizedlist>

<listitem><Para> tools/altperl/slon_watchdog.pl  - an "early" version that basically wraps a loop around the invocation of slon, restarting any time it  falls over
<listitem><Para> tools/altperl/slon_watchdog2.pl - a somewhat more intelligent version that periodically polls the database, checking to see if a SYNC has taken place recently.  We have had VPN connections that occasionally fall over without signalling the application, so that the slon stops working, but doesn't actually die; this polling accounts for that...
</itemizedlist>

<para>The <filename/slon_watchdog2.pl/ script is probably <emphasis/usually/ the preferable thing to run.  It was at one point not preferable to run it whilst subscribing a very large replication set where it is expected to take many hours to do the initial <command/COPY SET/.  The problem that came up in that case was that it figured that since it hasn't done a <command/SYNC/ in 2 hours, something was  broken requiring  restarting slon, thereby restarting the <command/COPY SET/ event.  More recently, the script has been changed to detect <command/COPY SET/ in progress.

</article>
<!-- Keep this comment at the end of the file
Local variables:
mode:sgml
sgml-omittag:nil
sgml-shorttag:t
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:1
sgml-indent-data:t
sgml-parent-document:nil
sgml-default-dtd-file:"./reference.ced"
sgml-exposed-tags:nil
sgml-local-catalogs:("/usr/lib/sgml/catalog")
sgml-local-ecat-files:nil
End:
-->
