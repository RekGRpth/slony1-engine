<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Slon daemons</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:cbbrowne@gmail.com"><LINK
REL="HOME"
TITLE="Slony-I 1.1 Administration"
HREF="slony.html"><LINK
REL="UP"
HREF="t24.html"><LINK
REL="PREVIOUS"
TITLE=" Slony-I Administration Scripts"
HREF="altperl.html"><LINK
REL="NEXT"
TITLE="Slon Configuration Options"
HREF="slonconfig.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stdstyle.css"><META
HTTP-EQUIV="Content-Type"></HEAD
><BODY
CLASS="SECT1"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>Slony-I 1.1 Administration</TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="altperl.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
></TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
><A
HREF="slonconfig.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="SLONSTART"
>9. Slon daemons</A
></H1
><P
>The programs that actually perform Slony-I replication are the
<B
CLASS="APPLICATION"
>slon</B
> daemons.&#13;</P
><P
>You need to run one <B
CLASS="APPLICATION"
>slon</B
> instance for each node in
a Slony-I cluster, whether you consider that node a <SPAN
CLASS="QUOTE"
>"master"</SPAN
> or
a <SPAN
CLASS="QUOTE"
>"slave."</SPAN
> Since a <TT
CLASS="COMMAND"
>MOVE SET</TT
> or <TT
CLASS="COMMAND"
>FAILOVER</TT
> can
switch the roles of nodes, slon needs to be able to function for both
providers and subscribers.  It is not essential that these daemons run
on any particular host, but there are some principles worth
considering:

<P
></P
><UL
><LI
><P
> Each slon needs to be able to communicate quickly
with the database whose <SPAN
CLASS="QUOTE"
>"node controller"</SPAN
> it is.  Therefore, if
a Slony-I cluster runs across some form of Wide Area Network, each
slon process should run on or nearby the databases each is
controlling.  If you break this rule, no particular disaster should
ensue, but the added latency introduced to monitoring events on the
slon's <SPAN
CLASS="QUOTE"
>"own node"</SPAN
> will cause it to replicate in a
<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>somewhat</I
></SPAN
> less timely manner.&#13;</P
></LI
><LI
><P
> The fastest results would be achieved by having each
slon run on the database server that it is servicing.  If it runs
somewhere within a fast local network, performance will not be
noticeably degraded.&#13;</P
></LI
><LI
><P
> It is an attractive idea to run many of the
<B
CLASS="APPLICATION"
>slon</B
> processes for a cluster on one machine, as this
makes it easy to monitor them both in terms of log files and process
tables from one location.  This eliminates the need to login to
several hosts in order to look at log files or to restart <B
CLASS="APPLICATION"
>slon</B
>
instances.&#13;</P
></LI
></UL
>&#13;</P
><P
>There are two <SPAN
CLASS="QUOTE"
>"watchdog"</SPAN
> scripts currently available:

<P
></P
><UL
><LI
><P
> <TT
CLASS="FILENAME"
>tools/altperl/slon_watchdog.pl</TT
> -
an <SPAN
CLASS="QUOTE"
>"early"</SPAN
> version that basically wraps a loop around the
invocation of <B
CLASS="APPLICATION"
>slon</B
>, restarting any time it falls over&#13;</P
></LI
><LI
><P
> <TT
CLASS="FILENAME"
>tools/altperl/slon_watchdog2.pl</TT
>
- a somewhat more intelligent version that periodically polls the
database, checking to see if a <TT
CLASS="COMMAND"
>SYNC</TT
> has taken place
recently.  We have had VPN connections that occasionally fall over
without signalling the application, so that the <B
CLASS="APPLICATION"
>slon</B
>
stops working, but doesn't actually die; this polling addresses that
issue.&#13;</P
></LI
></UL
>&#13;</P
><P
>The <TT
CLASS="FILENAME"
>slon_watchdog2.pl</TT
> script is probably
<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>usually</I
></SPAN
> the preferable thing to run.  It was at one point
not preferable to run it whilst subscribing a very large replication
set where it is expected to take many hours to do the initial
<TT
CLASS="COMMAND"
>COPY SET</TT
>.  The problem that came up in that case was that it
figured that since it hasn't done a <TT
CLASS="COMMAND"
>SYNC</TT
> in 2 hours,
something was broken requiring restarting slon, thereby restarting the
<TT
CLASS="COMMAND"
>COPY SET</TT
> event.  More recently, the script has been changed
to detect <TT
CLASS="COMMAND"
>COPY SET</TT
> in progress.


 </P
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="altperl.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="slony.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="slonconfig.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Slony-I Administration Scripts</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="t24.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>Slon Configuration Options</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>