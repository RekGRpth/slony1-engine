%META:TOPICINFO{author="guest" date="1099539982" format="1.0" version="1.6"}%
%META:TOPICPARENT{name="SlonyIAdministration"}%
---++ An Introduction to Slony-I

---+++ Why yet another replication system?

Slony-I was born from an idea to create a replication system that was not tied
to a specific version of PostgreSQL, which is allowed to be started and stopped on
an existing database with out the need for a dump/reload cycle.

---+++ What Slony-I is

Slony-I is a "master to multiple slaves" replication system supporting cascading and slave promotion.  The big picture for the development of Slony-I is as a  master-slave system that includes all features and capabilities needed to replicate large databases to a reasonably limited number of slave systems.  "Reasonable," in this context, is probably no more than a few dozen servers.  If the number of servers grows beyond that, the cost of communications becomes prohibitively high.  SlonyListenerCosts

Slony-I is a system intended for data centers and backup sites, where the normal mode of operation is that all nodes are available all the time, and where all nodes can be secured.  If you have nodes that are likely to regularly drop onto and off of the network, or have nodes that cannot be kept secure, Slony-I may not be the ideal replication solution for you.

There are plans for a "file-based log shipping" extension where updates would be serialized into files.  Given that, log files could be distributed by any means desired without any need of feedback between the provider node and those nodes subscribing via "log shipping."

---+++ What Slony-I is not

Slony-I is not a network management system.  

Slony-I does not have any functionality within it to detect a node failure, or automatically promote a node to a master or other data origin. 

Slony-I is not multi-master; it's not a connection broker, and it doesn't make you coffee and toast in the morning.

(That being said, the plan is for a subsequent system, Slony-II, to provide "multimaster" capabilities, and be "bootstrapped" using Slony-I.  But that is a separate project, and expectations for Slony-I should not be based on hopes for future projects.)

---+++ Why doesn't Slony-I do automatic fail-over/promotion?

This is the job of network monitoring software, not Slony.  Every site's configuration and fail-over path is different.  For example, keep-alive
monitoring with redundant NIC's and intelligent HA switches that guarantee race-condition-free takeover of a network address and disconnecting the
"failed" node vary in every network setup, vendor choice, hardware/software combination.  This is clearly the realm of network management software and not
Slony-I.  

Let Slony-I do what it does best: provide database replication.

---+++ Current Limitations

Slony-I does not automatically propagate schema changes, nor does it have any ability to replicate large objects.  There is a single common reason for these limitations, namely that Slony-I operates using triggers, and neither schema changes nor large object operations can raise triggers suitable to tell Slony-I when those kinds of changes take place.

There is a capability for Slony-I to propagate DDL changes if you submit them as scripts via the slonik EXECUTE SCRIPT operation.  That is not "automatic;" you have to construct an SQL DDL script and submit it.

If you have those sorts of requirements, it may be worth exploring the use of PostgreSQL 8.0 PITR (Point In Time Recovery), where WAL logs are replicated to remote nodes.  Unfortunately, that has two attendant limitations:

	* PITR replicates _all_ changes in _all_ databases; you cannot exclude data that isn't relevant;
	* A PITR replica remains dormant until you apply logs and start up the database.  You cannot use the database and apply updates simultaneously.  It is like having a "standby server" which cannot be used without it ceasing to be "standby."

There are a number of distinct models for database replication; it is impossible for one replication system to be all things to all people.
