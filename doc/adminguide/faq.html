<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Slony-I FAQ</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:cbbrowne@gmail.com"><LINK
REL="HOME"
TITLE="Slony-I 1.1 Administration"
HREF="slony.html"><LINK
REL="PREVIOUS"
TITLE=" More Slony-I Help "
HREF="help.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stdstyle.css"><META
HTTP-EQUIV="Content-Type"></HEAD
><BODY
CLASS="ARTICLE"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>Slony-I 1.1 Administration</TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="help.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
></TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
>&nbsp;</TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="ARTICLE"
><DIV
CLASS="TITLEPAGE"
><H1
CLASS="TITLE"
><A
NAME="AEN1025"
>Slony-I FAQ</A
></H1
><H3
CLASS="CORPAUTHOR"
>The Slony Global Development Group</H3
><H3
CLASS="AUTHOR"
><A
NAME="AEN1028"
>Christopher  Browne</A
></H3
><HR></DIV
><P
> Not all of these are, strictly speaking, <SPAN
CLASS="QUOTE"
>"frequently
asked;"</SPAN
> some represent <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>trouble found that seemed
worth documenting</I
></SPAN
>.</P
><DIV
CLASS="QANDASET"
><DL
><DT
>Q: <A
HREF="faq.html#AEN1036"
>I looked for the <CODE
CLASS="ENVAR"
>_clustername</CODE
> namespace, and
it wasn't there.</A
></DT
><DT
>Q: <A
HREF="faq.html#AEN1047"
>Some events moving around, but no replication&#13;</A
></DT
></DL
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1036"
></A
><B
>Q: I looked for the <CODE
CLASS="ENVAR"
>_clustername</CODE
> namespace, and
it wasn't there.</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> If the DSNs are wrong, then slon instances can't connect to the nodes.&#13;</P
><P
>This will generally lead to nodes remaining entirely untouched.&#13;</P
><P
>Recheck the connection configuration.  By the way, since
<B
CLASS="APPLICATION"
>slon</B
> links to libpq, you could have password information
stored in <TT
CLASS="FILENAME"
> <CODE
CLASS="ENVAR"
>$HOME</CODE
>/.pgpass</TT
>,
partially filling in right/wrong authentication information there.</P
></DIV
></DIV
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1047"
></A
><B
>Q: Some events moving around, but no replication&#13;</B
></BIG
></P
><P
> Slony logs might look like the following:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    DEBUG1 remoteListenThread_1: connected to 'host=host004 dbname=pgbenchrep user=postgres port=5432'
    ERROR  remoteListenThread_1: "select ev_origin, ev_seqno, ev_timestamp,		  ev_minxid, ev_maxxid, ev_xip,		  ev_type,		  ev_data1, ev_data2,		  ev_data3, ev_data4,		  ev_data5, ev_data6,		  ev_data7, ev_data8 from "_pgbenchtest".sl_event e where (e.ev_origin = '1' and e.ev_seqno &#62; '1') order by e.ev_origin, e.ev_seqno" - could not receive data from server: Operation now in progress</PRE
></TD
></TR
></TABLE
>&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>On AIX and Solaris (and possibly elsewhere), both Slony-I <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>and PostgreSQL</I
></SPAN
> must be compiled with the <CODE
CLASS="OPTION"
>--enable-thread-safety</CODE
> option.  The above results when PostgreSQL isn't so compiled.&#13;</P
><P
>What breaks here is that the libc (threadsafe) and libpq (non-threadsafe) use different memory locations for errno, thereby leading to the request failing.&#13;</P
><P
>Problems like this crop up with disadmirable regularity on AIX
and Solaris; it may take something of an <SPAN
CLASS="QUOTE"
>"object code audit"</SPAN
> to
make sure that <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>ALL</I
></SPAN
> of the necessary components have been
compiled and linked with <CODE
CLASS="OPTION"
>--enable-thread-safety</CODE
>.&#13;</P
><P
>For instance, I ran into the problem one that
<CODE
CLASS="ENVAR"
>LD_LIBRARY_PATH</CODE
> had been set, on Solaris, to point to
libraries from an old PostgreSQL compile.  That meant that even though
the database <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>had</I
></SPAN
> been compiled with
<CODE
CLASS="OPTION"
>--enable-thread-safety</CODE
>, and <B
CLASS="APPLICATION"
>slon</B
> had been
compiled against that, <B
CLASS="APPLICATION"
>slon</B
> was being dynamically linked
to the <SPAN
CLASS="QUOTE"
>"bad old thread-unsafe version,"</SPAN
> so slon didn't work.  It
wasn't clear that this was the case until I ran <TT
CLASS="COMMAND"
>ldd</TT
> against
<B
CLASS="APPLICATION"
>slon</B
>.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1070"
></A
><B
>Q: I tried creating a CLUSTER NAME with a "-" in it.
That didn't work.&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> Slony-I uses the same rules for unquoted identifiers as the PostgreSQL
main parser, so no, you probably shouldn't put a "-" in your
identifier name.&#13;</P
><P
> You may be able to defeat this by putting "quotes" around
identifier names, but it's liable to bite you some, so this is
something that is probably not worth working around.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1076"
></A
><B
>Q:  slon does not restart after crash&#13;</B
></BIG
></P
><P
> After an immediate stop of postgresql (simulation of system crash)
in pg_catalog.pg_listener a tuple with
relname='_${cluster_name}_Restart' exists. slon doesn't start cause it
thinks another process is serving the cluster on this node.  What can
I do? The tuples can't be dropped from this relation.&#13;</P
><P
> The logs claim that "Another slon daemon is serving this node already"&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>It's handy to keep a slonik script like the following one around to
run in such cases:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    twcsds004[/opt/twcsds004/OXRS/slony-scripts]$ cat restart_org.slonik 
    cluster name = oxrsorg ;
    node 1 admin conninfo = 'host=32.85.68.220 dbname=oxrsorg user=postgres port=5532';
    node 2 admin conninfo = 'host=32.85.68.216 dbname=oxrsorg user=postgres port=5532';
    node 3 admin conninfo = 'host=32.85.68.244 dbname=oxrsorg user=postgres port=5532';
    node 4 admin conninfo = 'host=10.28.103.132 dbname=oxrsorg user=postgres port=5532';
    restart node 1;
    restart node 2;
    restart node 3;
    restart node 4;</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
> <TT
CLASS="COMMAND"
>restart node n</TT
> cleans up dead notifications so that you can restart the node.&#13;</P
><P
>As of version 1.0.5, the startup process of slon looks for this
condition, and automatically cleans it up.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1087"
></A
><B
>Q: ps finds passwords on command line&#13;</B
></BIG
></P
><P
> If I run a <TT
CLASS="COMMAND"
>ps</TT
> command, I, and everyone else, can see passwords
on the command line.&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>Take the passwords out of the Slony configuration, and put them into
<TT
CLASS="FILENAME"
><CODE
CLASS="ENVAR"
>$(HOME)</CODE
>/.pgpass.</TT
>&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1096"
></A
><B
>Q: Slonik fails - cannot load PostgreSQL library - <TT
CLASS="COMMAND"
>PGRES_FATAL_ERROR load '$libdir/xxid';</TT
>&#13;</B
></BIG
></P
><P
> When I run the sample setup script I get an error message similar
to:

<TT
CLASS="COMMAND"
>stdin:64: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  LOAD:
could not open file '$libdir/xxid': No such file or directory</TT
>&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> Evidently, you haven't got the <TT
CLASS="FILENAME"
>xxid.so</TT
>
library in the <CODE
CLASS="ENVAR"
>$libdir</CODE
> directory that the PostgreSQL instance
is using.  Note that the Slony-I components need to be installed in
the PostgreSQL software installation for <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>each and every one</I
></SPAN
>
of the nodes, not just on the <SPAN
CLASS="QUOTE"
>"master node."</SPAN
>&#13;</P
><P
>This may also point to there being some other mismatch between
the PostgreSQL binary instance and the Slony-I instance.  If you
compiled Slony-I yourself, on a machine that may have multiple
PostgreSQL builds <SPAN
CLASS="QUOTE"
>"lying around,"</SPAN
> it's possible that the slon or
slonik binaries are asking to load something that isn't actually in
the library directory for the PostgreSQL database cluster that it's
hitting.&#13;</P
><P
>Long and short: This points to a need to <SPAN
CLASS="QUOTE"
>"audit"</SPAN
> what
installations of PostgreSQL and Slony you have in place on the
machine(s).  Unfortunately, just about any mismatch will cause things
not to link up quite right.  See also <A
HREF="faq.html#SLONYFAQ02"
>SlonyFAQ02 </A
> concerning threading issues on Solaris ...&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1113"
></A
><B
>Q: Table indexes with FQ namespace names

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    set add table (set id = 1, origin = 1, id = 27, 
                   full qualified name = 'nspace.some_table', 
                   key = 'key_on_whatever', 
                   comment = 'Table some_table in namespace nspace with a candidate primary key');</PRE
></TD
></TR
></TABLE
>&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> If you have <TT
CLASS="COMMAND"
> key = 'nspace.key_on_whatever'</TT
>
the request will <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>FAIL</I
></SPAN
>.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1121"
></A
><B
>Q: I'm trying to get a slave subscribed, and get the following
messages in the logs:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    DEBUG1 copy_set 1
    DEBUG1 remoteWorkerThread_1: connected to provider DB
    WARN	remoteWorkerThread_1: transactions earlier than XID 127314958 are still in progress
    WARN	remoteWorkerThread_1: data copy for set 1 failed - sleep 60 seconds</PRE
></TD
></TR
></TABLE
>&#13;</B
></BIG
></P
><P
>Oops.  What I forgot to mention, as well, was that I was trying
to add <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>TWO</I
></SPAN
> subscribers, concurrently.&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> That doesn't work out: Slony-I won't work on the
<TT
CLASS="COMMAND"
>COPY</TT
> commands concurrently.  See
<TT
CLASS="FILENAME"
>src/slon/remote_worker.c</TT
>, function
<CODE
CLASS="FUNCTION"
>copy_set()</CODE
>&#13;</P
><P
>This has the (perhaps unfortunate) implication that you cannot
populate two slaves concurrently.  You have to subscribe one to the
set, and only once it has completed setting up the subscription
(copying table contents and such) can the second subscriber start
setting up the subscription.&#13;</P
><P
>It could also be possible for there to be an old outstanding
transaction blocking Slony-I from processing the sync.  You might want
to take a look at pg_locks to see what's up:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    sampledb=# select * from pg_locks where transaction is not null order by transaction;
     relation | database | transaction |  pid    |     mode      | granted 
    ----------+----------+-------------+---------+---------------+---------
              |          |   127314921 | 2605100 | ExclusiveLock | t
              |          |   127326504 | 5660904 | ExclusiveLock | t
    (2 rows)</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>See?  127314921 is indeed older than 127314958, and it's still running.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    $ ps -aef | egrep '[2]605100'
    
    postgres 2605100  205018	0 18:53:43  pts/3  3:13 postgres: postgres sampledb localhost COPY </PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>This happens to be a <TT
CLASS="COMMAND"
>COPY</TT
> transaction involved in setting up the
subscription for one of the nodes.  All is well; the system is busy
setting up the first subscriber; it won't start on the second one
until the first one has completed subscribing.&#13;</P
><P
>By the way, if there is more than one database on the PostgreSQL
cluster, and activity is taking place on the OTHER database, that will
lead to there being <SPAN
CLASS="QUOTE"
>"transactions earlier than XID whatever"</SPAN
> being
found to be still in progress.  The fact that it's a separate database
on the cluster is irrelevant; Slony-I will wait until those old
transactions terminate.</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1141"
></A
><B
>Q: ERROR: duplicate key violates unique constraint "sl_table-pkey"&#13;</B
></BIG
></P
><P
>I tried setting up a second replication set, and got the following error:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    stdin:9: Could not create subscription set 2 for oxrslive!
    stdin:11: PGRES_FATAL_ERROR select "_oxrslive".setAddTable(2, 1, 'public.replic_test', 'replic_test__Slony-I_oxrslive_rowID_key', 'Table public.replic_test without primary key');  - ERROR:  duplicate key violates unique constraint "sl_table-pkey"
    CONTEXT:  PL/pgSQL function "setaddtable_int" line 71 at SQL statement</PRE
></TD
></TR
></TABLE
>&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>The table IDs used in SET ADD TABLE are required to be unique <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>ACROSS
ALL SETS</I
></SPAN
>.  Thus, you can't restart numbering at 1 for a second set; if
you are numbering them consecutively, a subsequent set has to start
with IDs after where the previous set(s) left off.</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1149"
></A
><B
>Q: I need to drop a table from a replication set</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>This can be accomplished several ways, not all equally desirable ;-).

<P
></P
><UL
><LI
><P
> You could drop the whole replication set, and recreate it with just the tables that you need.  Alas, that means recopying a whole lot of data, and kills the usability of the cluster on the rest of the set while that's happening.&#13;</P
></LI
><LI
><P
> If you are running 1.0.5 or later, there is the command SET DROP TABLE, which will "do the trick."&#13;</P
></LI
><LI
><P
> If you are still using 1.0.1 or 1.0.2, the _essential_ functionality of SET DROP TABLE involves the functionality in droptable_int().  You can fiddle this by hand by finding the table ID for the table you want to get rid of, which you can find in sl_table, and then run the following three queries, on each host:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>      select _slonyschema.alterTableRestore(40);
      select _slonyschema.tableDropKey(40);
      delete from _slonyschema.sl_table where tab_id = 40;</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>The schema will obviously depend on how you defined the Slony-I
cluster.  The table ID, in this case, 40, will need to change to the
ID of the table you want to have go away.

You'll have to run these three queries on all of the nodes, preferably
firstly on the "master" node, so that the dropping of this propagates
properly.  Implementing this via a SLONIK statement with a new Slony
event would do that.  Submitting the three queries using EXECUTE
SCRIPT could do that.  Also possible would be to connect to each
database and submit the queries by hand.</P
></LI
></UL
></P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1163"
></A
><B
>Q: I need to drop a sequence from a replication set&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
></P
><P
>If you are running 1.0.5 or later, there is a
<TT
CLASS="COMMAND"
>SET DROP SEQUENCE</TT
> command in Slonik to allow you to do this,
parallelling <TT
CLASS="COMMAND"
>SET DROP TABLE.</TT
>&#13;</P
><P
>If you are running 1.0.2 or earlier, the process is a bit more manual.&#13;</P
><P
>Supposing I want to get rid of the two sequences listed below,
<CODE
CLASS="ENVAR"
>whois_cachemgmt_seq</CODE
> and <CODE
CLASS="ENVAR"
>epp_whoi_cach_seq_</CODE
>, we start
by needing the <CODE
CLASS="ENVAR"
>seq_id</CODE
> values.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    oxrsorg=# select * from _oxrsorg.sl_sequence  where seq_id in (93,59);
     seq_id | seq_reloid | seq_set |       seq_comment				 
    --------+------------+---------+-------------------------------------
         93 |  107451516 |       1 | Sequence public.whois_cachemgmt_seq
         59 |  107451860 |       1 | Sequence public.epp_whoi_cach_seq_
    (2 rows)</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>The data that needs to be deleted to stop Slony from continuing to
replicate these are thus:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    delete from _oxrsorg.sl_seqlog where seql_seqid in (93, 59);
    delete from _oxrsorg.sl_sequence where seq_id in (93,59);</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>Those two queries could be submitted to all of the nodes via
<CODE
CLASS="FUNCTION"
>ddlscript()</CODE
> / <TT
CLASS="COMMAND"
>EXECUTE SCRIPT</TT
>, thus eliminating
the sequence everywhere <SPAN
CLASS="QUOTE"
>"at once."</SPAN
> Or they may be applied by
hand to each of the nodes.&#13;</P
><P
>Similarly to <TT
CLASS="COMMAND"
>SET DROP TABLE</TT
>, this should be in place for Slony-I version
1.0.5 as <TT
CLASS="COMMAND"
>SET DROP SEQUENCE.</TT
></P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1186"
></A
><B
>Q: Slony-I: cannot add table to currently subscribed set 1&#13;</B
></BIG
></P
><P
> I tried to add a table to a set, and got the following message:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    	Slony-I: cannot add table to currently subscribed set 1</PRE
></TD
></TR
></TABLE
>&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> You cannot add tables to sets that already have
subscribers.&#13;</P
><P
>The workaround to this is to create <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>ANOTHER</I
></SPAN
> set, add
the new tables to that new set, subscribe the same nodes subscribing
to "set 1" to the new set, and then merge the sets together.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1195"
></A
><B
>Q: Some nodes start consistently falling behind&#13;</B
></BIG
></P
><P
>I have been running Slony-I on a node for a while, and am seeing
system performance suffering.&#13;</P
><P
>I'm seeing long running queries of the form:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    	fetch 100 from LOG;</PRE
></TD
></TR
></TABLE
>&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> This is characteristic of pg_listener (which is the table containing
<TT
CLASS="COMMAND"
>NOTIFY</TT
> data) having plenty of dead tuples in it.  That makes <TT
CLASS="COMMAND"
>NOTIFY</TT
>
events take a long time, and causes the affected node to gradually
fall further and further behind.&#13;</P
><P
>You quite likely need to do a <TT
CLASS="COMMAND"
>VACUUM FULL</TT
> on <CODE
CLASS="ENVAR"
>pg_listener</CODE
>, to vigorously clean it out, and need to vacuum <CODE
CLASS="ENVAR"
>pg_listener</CODE
> really frequently.  Once every five minutes would likely be AOK.&#13;</P
><P
> Slon daemons already vacuum a bunch of tables, and
<TT
CLASS="FILENAME"
>cleanup_thread.c</TT
> contains a list of tables that are
frequently vacuumed automatically.  In Slony-I 1.0.2,
<CODE
CLASS="ENVAR"
>pg_listener</CODE
> is not included.  In 1.0.5 and later, it is
regularly vacuumed, so this should cease to be a direct issue.&#13;</P
><P
>There is, however, still a scenario where this will still
"bite."  Vacuums cannot delete tuples that were made "obsolete" at any
time after the start time of the eldest transaction that is still
open.  Long running transactions will cause trouble, and should be
avoided, even on "slave" nodes.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1213"
></A
><B
>Q: I started doing a backup using pg_dump, and suddenly Slony stops&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>Ouch.  What happens here is a conflict between:
<P
></P
><UL
><LI
><P
> <B
CLASS="APPLICATION"
>pg_dump</B
>, which has taken out an <TT
CLASS="COMMAND"
>AccessShareLock</TT
> on all of the tables in the database, including the Slony-I ones, and&#13;</P
></LI
><LI
><P
> A Slony-I sync event, which wants to grab a <TT
CLASS="COMMAND"
>AccessExclusiveLock</TT
> on	 the table <CODE
CLASS="ENVAR"
>sl_event</CODE
>.</P
></LI
></UL
>&#13;</P
><P
>The initial query that will be blocked is thus:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    select "_slonyschema".createEvent('_slonyschema, 'SYNC', NULL);	  </PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>(You can see this in <CODE
CLASS="ENVAR"
>pg_stat_activity</CODE
>, if you have query
display turned on in <TT
CLASS="FILENAME"
>postgresql.conf</TT
>)&#13;</P
><P
>The actual query combination that is causing the lock is from
the function <CODE
CLASS="FUNCTION"
>Slony_I_ClusterStatus()</CODE
>, found in
<TT
CLASS="FILENAME"
>slony1_funcs.c</TT
>, and is localized in the code that does:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>      LOCK TABLE %s.sl_event;
      INSERT INTO %s.sl_event (...stuff...)
      SELECT currval('%s.sl_event_seq');</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>The <TT
CLASS="COMMAND"
>LOCK</TT
> statement will sit there and wait until <TT
CLASS="COMMAND"
>pg_dump</TT
> (or whatever else has pretty much any kind of access lock on <CODE
CLASS="ENVAR"
>sl_event</CODE
>) completes.  &#13;</P
><P
>Every subsequent query submitted that touches <CODE
CLASS="ENVAR"
>sl_event</CODE
> will block behind the <CODE
CLASS="FUNCTION"
>createEvent</CODE
> call.&#13;</P
><P
>There are a number of possible answers to this:
<P
></P
><UL
><LI
><P
> Have pg_dump specify the schema dumped using
--schema=whatever, and don't try dumping the cluster's schema.&#13;</P
></LI
><LI
><P
> It would be nice to add an "--exclude-schema" option
to pg_dump to exclude the Slony cluster schema.  Maybe in 8.0 or
8.1...&#13;</P
></LI
><LI
><P
>Note that 1.0.5 uses a more precise lock that is less
exclusive that alleviates this problem.</P
></LI
></UL
></P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1251"
></A
><B
>Q: The slons spent the weekend out of commission [for
some reason], and it's taking a long time to get a sync through.&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>You might want to take a look at the sl_log_1/sl_log_2 tables, and do
a summary to see if there are any really enormous Slony-I transactions
in there.  Up until at least 1.0.2, there needs to be a slon connected
to the master in order for <TT
CLASS="COMMAND"
>SYNC</TT
> events to be generated.&#13;</P
><P
>If none are being generated, then all of the updates until the next
one is generated will collect into one rather enormous Slony-I
transaction.&#13;</P
><P
>Conclusion: Even if there is not going to be a subscriber around, you
<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>really</I
></SPAN
> want to have a slon running to service the <SPAN
CLASS="QUOTE"
>"master"</SPAN
> node.&#13;</P
><P
>Some future version (probably 1.1) may provide a way for
<TT
CLASS="COMMAND"
>SYNC</TT
> counts to be updated on the master by the stored
function that is invoked by the table triggers.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1263"
></A
><B
>Q: I pointed a subscribing node to a different parent and it stopped replicating&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
>We noticed this happening when we wanted to re-initialize a node,
where we had configuration thus:

<P
></P
><UL
><LI
><P
> Node 1 - master</P
></LI
><LI
><P
> Node 2 - child of node 1 - the node we're reinitializing</P
></LI
><LI
><P
> Node 3 - child of node 3 - node that should keep replicating</P
></LI
></UL
>&#13;</P
><P
>The subscription for node 3 was changed to have node 1 as
provider, and we did <TT
CLASS="COMMAND"
>DROP SET</TT
>/<TT
CLASS="COMMAND"
>SUBSCRIBE SET</TT
> for
node 2 to get it repopulating.&#13;</P
><P
>Unfortunately, replication suddenly stopped to node 3.&#13;</P
><P
>The problem was that there was not a suitable set of <SPAN
CLASS="QUOTE"
>"listener paths"</SPAN
>
in sl_listen to allow the events from node 1 to propagate to node 3.
The events were going through node 2, and blocking behind the
<TT
CLASS="COMMAND"
>SUBSCRIBE SET</TT
> event that node 2 was working on.&#13;</P
><P
>The following slonik script dropped out the listen paths where node 3
had to go through node 2, and added in direct listens between nodes 1
and 3.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    cluster name = oxrslive;
     node 1 admin conninfo='host=32.85.68.220 dbname=oxrslive user=postgres port=5432';
     node 2 admin conninfo='host=32.85.68.216 dbname=oxrslive user=postgres port=5432';
     node 3 admin conninfo='host=32.85.68.244 dbname=oxrslive user=postgres port=5432';
     node 4 admin conninfo='host=10.28.103.132 dbname=oxrslive user=postgres port=5432';
    try {
      store listen (origin = 1, receiver = 3, provider = 1);
      store listen (origin = 3, receiver = 1, provider = 3);
      drop listen (origin = 1, receiver = 3, provider = 2);
      drop listen (origin = 3, receiver = 1, provider = 2);
    }</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>Immediately after this script was run, <TT
CLASS="COMMAND"
>SYNC</TT
> events started propagating
again to node 3.

This points out two principles:
<P
></P
><UL
><LI
><P
> If you have multiple nodes, and cascaded subscribers,
you need to be quite careful in populating the <TT
CLASS="COMMAND"
>STORE LISTEN</TT
>
entries, and in modifying them if the structure of the replication
"tree" changes.&#13;</P
></LI
><LI
><P
> Version 1.1 probably ought to provide better tools to
help manage this.&#13;</P
></LI
></UL
>&#13;</P
><P
>The issues of "listener paths" are discussed further at <A
HREF="listenpaths.html"
> Slony Listen Paths </A
>&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1294"
></A
><B
>Q: After dropping a node, sl_log_1 isn't getting purged out anymore.&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> This is a common scenario in versions before 1.0.5, as
the "clean up" that takes place when purging the node does not include
purging out old entries from the Slony-I table, sl_confirm, for the
recently departed node.&#13;</P
><P
> The node is no longer around to update confirmations of what
syncs have been applied on it, and therefore the cleanup thread that
purges log entries thinks that it can't safely delete entries newer
than the final sl_confirm entry, which rather curtails the ability to
purge out old logs.&#13;</P
><P
>Diagnosis: Run the following query to see if there are any
"phantom/obsolete/blocking" sl_confirm entries:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    oxrsbar=# select * from _oxrsbar.sl_confirm where con_origin not in (select no_id from _oxrsbar.sl_node) or con_received not in (select no_id from _oxrsbar.sl_node);
     con_origin | con_received | con_seqno |        con_timestamp                  
    ------------+--------------+-----------+----------------------------
              4 |          501 |     83999 | 2004-11-09 19:57:08.195969
              1 |            2 |   3345790 | 2004-11-14 10:33:43.850265
              2 |          501 |    102718 | 2004-11-14 10:33:47.702086
            501 |            2 |      6577 | 2004-11-14 10:34:45.717003
              4 |            5 |     83999 | 2004-11-14 21:11:11.111686
              4 |            3 |     83999 | 2004-11-24 16:32:39.020194
    (6 rows)</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>In version 1.0.5, the "drop node" function purges out entries in
sl_confirm for the departing node.  In earlier versions, this needs to
be done manually.  Supposing the node number is 3, then the query
would be:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    delete from _namespace.sl_confirm where con_origin = 3 or con_received = 3;</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>Alternatively, to go after <SPAN
CLASS="QUOTE"
>"all phantoms,"</SPAN
> you could use
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    oxrsbar=# delete from _oxrsbar.sl_confirm where con_origin not in (select no_id from _oxrsbar.sl_node) or con_received not in (select no_id from _oxrsbar.sl_node);
    DELETE 6</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>General <SPAN
CLASS="QUOTE"
>"due diligance"</SPAN
> dictates starting with a
<TT
CLASS="COMMAND"
>BEGIN</TT
>, looking at the contents of sl_confirm before,
ensuring that only the expected records are purged, and then, only
after that, confirming the change with a <TT
CLASS="COMMAND"
>COMMIT</TT
>.  If you
delete confirm entries for the wrong node, that could ruin your whole
day.&#13;</P
><P
>You'll need to run this on each node that remains...&#13;</P
><P
>Note that in 1.0.5, this is no longer an issue at all, as it purges unneeded entries from sl_confirm in two places:
<P
></P
><UL
><LI
><P
> At the time a node is dropped</P
></LI
><LI
><P
> At the start of each "cleanupEvent" run, which is the event in which old data is purged from sl_log_1 and sl_seqlog</P
></LI
></UL
>&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1318"
></A
><B
>Q: Replication Fails - Unique Constraint Violation&#13;</B
></BIG
></P
><P
>Replication has been running for a while, successfully, when a
node encounters a "glitch," and replication logs are filled with
repetitions of the following:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="SCREEN"
>    DEBUG2 remoteWorkerThread_1: syncing set 2 with 5 table(s) from provider 1
    DEBUG2 remoteWorkerThread_1: syncing set 1 with 41 table(s) from provider 1
    DEBUG2 remoteWorkerThread_1: syncing set 5 with 1 table(s) from provider 1
    DEBUG2 remoteWorkerThread_1: syncing set 3 with 1 table(s) from provider 1
    DEBUG2 remoteHelperThread_1_1: 0.135 seconds delay for first row
    DEBUG2 remoteHelperThread_1_1: 0.343 seconds until close cursor
    ERROR  remoteWorkerThread_1: "insert into "_oxrsapp".sl_log_1          (log_origin, log_xid, log_tableid,                log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '34', '35090538', 'D', '_rserv_ts=''9275244''');
    delete from only public.epp_domain_host where _rserv_ts='9275244';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '34', '35090539', 'D', '_rserv_ts=''9275245''');
    delete from only public.epp_domain_host where _rserv_ts='9275245';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '26', '35090540', 'D', '_rserv_ts=''24240590''');
    delete from only public.epp_domain_contact where _rserv_ts='24240590';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '26', '35090541', 'D', '_rserv_ts=''24240591''');
    delete from only public.epp_domain_contact where _rserv_ts='24240591';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '26', '35090542', 'D', '_rserv_ts=''24240589''');
    delete from only public.epp_domain_contact where _rserv_ts='24240589';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '11', '35090543', 'D', '_rserv_ts=''36968002''');
    delete from only public.epp_domain_status where _rserv_ts='36968002';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '11', '35090544', 'D', '_rserv_ts=''36968003''');
    delete from only public.epp_domain_status where _rserv_ts='36968003';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '24', '35090549', 'I', '(contact_id,status,reason,_rserv_ts) values (''6972897'',''64'','''',''31044208'')');
    insert into public.contact_status (contact_id,status,reason,_rserv_ts) values ('6972897','64','','31044208');insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '24', '35090550', 'D', '_rserv_ts=''18139332''');
    delete from only public.contact_status where _rserv_ts='18139332';insert into "_oxrsapp".sl_log_1	  (log_origin, log_xid, log_tableid,		log_actionseq, log_cmdtype,		log_cmddata) values	  ('1', '919151224', '24', '35090551', 'D', '_rserv_ts=''18139333''');
    delete from only public.contact_status where _rserv_ts='18139333';" ERROR:  duplicate key violates unique constraint "contact_status_pkey"
     - qualification was: 
    ERROR  remoteWorkerThread_1: SYNC aborted</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>The transaction rolls back, and Slony-I tries again, and again,
and again.  The problem is with one of the <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>last</I
></SPAN
> SQL statements, the
one with <TT
CLASS="COMMAND"
>log_cmdtype = 'I'</TT
>.  That isn't quite obvious; what takes
place is that Slony-I groups 10 update queries together to diminish
the number of network round trips.&#13;</P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
></P
><P
> A <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>certain</I
></SPAN
> cause for this has not yet been arrived
at.  The factors that <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>appear</I
></SPAN
> to go together to contribute
to this scenario are as follows:

<P
></P
><UL
><LI
><P
> The "glitch" seems to coincide with some sort of
outage; it has been observed both in cases where databases were
suffering from periodic "SIG 11" problems, where backends were falling
over, as well as when temporary network failure seemed likely.&#13;</P
></LI
><LI
><P
> The scenario seems to involve a delete transaction
having been missed by Slony-I.&#13;</P
></LI
></UL
>&#13;</P
><P
>By the time we notice that there is a problem, the missed delete
transaction has been cleaned out of sl_log_1, so there is no recovery
possible.&#13;</P
><P
>What is necessary, at this point, is to drop the replication set
(or even the node), and restart replication from scratch on that node.&#13;</P
><P
>In Slony-I 1.0.5, the handling of purges of sl_log_1 are rather
more conservative, refusing to purge entries that haven't been
successfully synced for at least 10 minutes on all nodes.  It is not
certain that that will prevent the "glitch" from taking place, but it
seems likely that it will leave enough sl_log_1 data to be able to do
something about recovering from the condition or at least diagnosing
it more exactly.  And perhaps the problem is that sl_log_1 was being
purged too aggressively, and this will resolve the issue completely.&#13;</P
><DIV
CLASS="QANDAENTRY"
><DIV
CLASS="QUESTION"
><P
><BIG
><A
NAME="AEN1339"
></A
><B
>Q:  If you have a slonik script something like this, it
will hang on you and never complete, because you can't have
<TT
CLASS="COMMAND"
>wait for event</TT
> inside a <TT
CLASS="COMMAND"
>try</TT
> block. A <TT
CLASS="COMMAND"
>try</TT
>
block is executed as one transaction, and the event that you are
waiting for can never arrive inside the scope of the transaction.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    try {
          echo 'Moving set 1 to node 3';
          lock set (id=1, origin=1);
          echo 'Set locked';
          wait for event (origin = 1, confirmed = 3);
          echo 'Moving set';
          move set (id=1, old origin=1, new origin=3);
          echo 'Set moved - waiting for event to be confirmed by node 3';
          wait for event (origin = 1, confirmed = 3);
          echo 'Confirmed';
    } on error {
          echo 'Could not move set for cluster foo';
          unlock set (id=1, origin=1);
          exit -1;
    }</PRE
></TD
></TR
></TABLE
>&#13;</B
></BIG
></P
></DIV
><DIV
CLASS="ANSWER"
><P
><B
>A: </B
> You must not invoke <TT
CLASS="COMMAND"
>wait for event</TT
> inside a
<SPAN
CLASS="QUOTE"
>"try"</SPAN
> block.&#13;</P
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="help.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="slony.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>&nbsp;</TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>More Slony-I Help</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
>&nbsp;</TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>&nbsp;</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>