<!-- $Id: intro.sgml,v 1.11 2005-02-18 22:43:34 cbbrowne Exp $ -->
<sect1 id="introduction">
<title>Introduction to &slony1;</title>
<sect2><title>Why yet another replication system?</title>

<para>&slony1; was born from an idea to
create a replication system that was not tied to a specific version of
&postgres;, which is allowed to be started and stopped on an existing
database with out the need for a dump/reload cycle.</para> </sect2>

<sect2> <title>What &slony1; is</title>

<para>&slony1; is a <quote>master to
multiple slaves</quote> replication system supporting cascading and
slave promotion.  The big picture for the development of
&slony1; is as a master-slave system that
includes all features and capabilities needed to replicate large
databases to a reasonably limited number of slave systems.
<quote>Reasonable,</quote> in this context, is probably no more than a
few dozen servers.  If the number of servers grows beyond that, the
cost of communications becomes prohibitively high.</para>

<para> See also <xref linkend="slonylistenercosts"> for a further
analysis of costs associated with having many nodes.</para>

<para> &slony1; is a system intended for data centers and backup
sites, where the normal mode of operation is that all nodes are
available all the time, and where all nodes can be secured.  If you
have nodes that are likely to regularly drop onto and off of the
network, or have nodes that cannot be kept secure, &slony1; is
probably not the ideal replication solution for you.</para>

<para> Thus, examples of cases where &slony1; probably won't work out
well would include:

<itemizedlist>
<listitem><para> Sites where connectivity is really <quote>flakey</quote>
</para></listitem>
<listitem><para> Replication to nodes that are unpredictably connected.</para>

<para> Replicating a pricing database from a central server to sales
staff who connect periodically to grab updates.  </para></listitem>
</itemizedlist></para>

<para> There are plans for a <quote>file-based log shipping</quote>
extension where updates would be serialized into files.  Given that,
log files could be distributed by any means desired without any need
of feedback between the provider node and those nodes subscribing via
<quote>log shipping.</quote></para>

<para> But &slony1;, by only having a single origin for each set, is
quite unsuitable for <emphasis>really</emphasis> asynchronous multiway
replication.  For those that could use some sort of
<quote>asynchronous multimaster replication with conflict
resolution</quote> akin to what is provided by <productname>Lotus
<trademark>Notes</trademark></productname> or the
<quote>syncing</quote> protocols found on PalmOS systems, you will
really need to look elsewhere.  These sorts of replication models are
not without merit, but they represent <emphasis>different</emphasis>
replication scenarios that &slony1; does not attempt to
address.</para>

</sect2>

<sect2><title> What &slony1; is not</title>

<para>&slony1; is not a network management system.</para>

<para> &slony1; does not have any functionality within it to detect a
node failure, nor to automatically promote a node to a master or other
data origin.  It is quite possible that you may need to do that; that
will require that you combine some network tools that evaluate
<emphasis> to your satisfaction </emphasis> which nodes you consider
<quote>live</quote> and which nodes you consider <quote>dead</quote>
along with some local policy to determine what to do under those
circumstances.  &slony1; does not dictate any of that policy to
you.</para>

<para>&slony1; is not multi-master; it is not a connection broker, and
it doesn't make you coffee and toast in the morning.</para>

<para>That being said, the plan is for a subsequent system,
<productname>Slony-II</productname>, to provide
<quote>multimaster</quote> capabilities.  But that is a separate
project, and expectations for &slony1; should not be based on hopes
for future projects.</para></sect2>

<sect2><title> Why doesn't &slony1; do automatic fail-over/promotion?
</title>

<para>This is the job of network monitoring software, not &slony1;.
Every site's configuration and fail-over paths are different.  For
example, keep-alive monitoring with redundant NIC's and intelligent HA
switches that guarantee race-condition-free takeover of a network
address and disconnecting the <quote>failed</quote> node will vary
based on network configuration, vendor choices, and the combinations
of hardware and software in use.  This is clearly the realm of network
management software and not &slony1;.</para>

<para> Furthermore, choosing what to do based on the
<quote>shape</quote> of the cluster represents local business policy.
If &slony1; imposed failover policy on you,
that might conflict with business requirements, thereby making
&slony1; an unacceptable choice.</para>

<para>As a result, let &slony1; do what it does best: provide database
replication.</para></sect2>

<sect2><title> Current Limitations</title>

<para>&slony1; does not automatically
propagate schema changes, nor does it have any ability to replicate
large objects.  There is a single common reason for these limitations,
namely that &slony1; operates using
triggers, and neither schema changes nor large object operations can
raise triggers suitable to tell &slony1;
when those kinds of changes take place.</para>

<para>There is a capability for &slony1; to propagate DDL changes if
you submit them as scripts via the <application>slonik</application>
<xref linkend="stmtddlscript"> operation.  That is not
<quote>automatic;</quote> you have to construct an SQL DDL script and
submit it.</para>

<para>If you have those sorts of requirements, it may be worth
exploring the use of &postgres; 8.0 <acronym>PITR</acronym> (Point In
Time Recovery), where <acronym>WAL</acronym> logs are replicated to
remote nodes.  Unfortunately, that has two attendant limitations:

<itemizedlist>
	
<listitem><para> PITR replicates <emphasis>all</emphasis> changes in
<emphasis>all</emphasis> databases; you cannot exclude data that isn't
relevant;</para></listitem>

<listitem><para> A PITR replica remains dormant until you apply logs
and start up the database.  You cannot use the database and apply
updates simultaneously.  It is like having a <quote>standby
server</quote> which cannot be used without it ceasing to be
<quote>standby.</quote></para></listitem>

</itemizedlist></para>

<para>There are a number of distinct models for database replication;
it is impossible for one replication system to be all things to all
people.</para></sect2>

<sect1 id="slonylistenercosts"><title>
<productname>Slony-I</productname> Communications Costs</title>

<para>The cost of communications grows in a quadratic fashion in
several directions as the number of replication nodes in a cluster
increases.  Note the following relationships:

<itemizedlist>

<listitem><para> It is necessary to have a <xref linkend=
"table.sl-path"> entry allowing connection from each node to every
other node.  Most will normally not need to be used for a given
replication configuration, but this means that there needs to be
n(n-1) paths.  It is probable that there will be considerable
repetition of entries, since the path to <quote>node n</quote> is
likely to be the same from everywhere throughout the
network.</para></listitem>

<listitem><para> It is similarly necessary to have a <xref linkend=
"table.sl-listen"> entry indicating how data flows from every node to
every other node.  This again requires configuring n(n-1)
<quote>listener paths.</quote></para></listitem>

<listitem><para> Each SYNC applied needs to be reported back to all of
the other nodes participating in the set so that the nodes all know
that it is safe to purge <envar>sl_log_1</envar> and
<envar>sl_log_2</envar> data, as any <quote>forwarding</quote> node
could potentially take over as <quote>master</quote> at any time.  One
might expect SYNC messages to need to travel through n/2 nodes to get
propagated to their destinations; this means that each SYNC is
expected to get transmitted n(n/2) times.  Again, this points to a
quadratic growth in communications costs as the number of nodes
increases.</para></listitem>

</itemizedlist></para>

<para>This points to it being a bad idea to have the large
communications network resulting from the number of nodes being large.
Up to a half dozen nodes seems pretty reasonable; every time the
number of nodes doubles, this can be expected to quadruple
communications overheads.</para>
</sect1>

<!-- Keep this comment at the end of the file
Local variables:
mode:sgml
sgml-omittag:nil
sgml-shorttag:t
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:1
sgml-indent-data:t
sgml-parent-document:"book.sgml"
sgml-exposed-tags:nil
sgml-local-catalogs:("/usr/lib/sgml/catalog")
sgml-local-ecat-files:nil
End:
-->
