<article id="slonyintroduction"> <title/Introduction to Slony-I/

<sect1> <title>Why yet another replication system? </title>

<para>Slony-I was born from an idea to create a replication system that was not tied
to a specific version of PostgreSQL, which is allowed to be started and stopped on
an existing database with out the need for a dump/reload cycle.

<sect1> <title/What Slony-I is/

<para>Slony-I is a <quote/master to multiple slaves/ replication
system supporting cascading and slave promotion.  The big picture for
the development of Slony-I is as a master-slave system that includes
all features and capabilities needed to replicate large databases to a
reasonably limited number of slave systems.  "Reasonable," in this
context, is probably no more than a few dozen servers.  If the number
of servers grows beyond that, the cost of communications becomes
prohibitively high.  

<!-- See also <link linkend="SlonyListenerCosts"> SlonyListenerCosts </link> -->

<para> Slony-I is a system intended for data centers and backup sites,
where the normal mode of operation is that all nodes are available all
the time, and where all nodes can be secured.  If you have nodes that
are likely to regularly drop onto and off of the network, or have
nodes that cannot be kept secure, Slony-I may not be the ideal
replication solution for you.

<para> There are plans for a <quote/file-based log shipping/ extension where updates would be serialized into files.  Given that, log files could be distributed by any means desired without any need of feedback between the provider node and those nodes subscribing via "log shipping."


<sect1><title/ Slony-I is not/

<para>Slony-I is not a network management system.  

<para> Slony-I does not have any functionality within it to detect a
node failure, or automatically promote a node to a master or other
data origin.

<para>Slony-I is not multi-master; it's not a connection broker, and
it doesn't make you coffee and toast in the morning.

<para>(That being said, the plan is for a subsequent system, Slony-II,
to provide "multimaster" capabilities, and be "bootstrapped" using
Slony-I.  But that is a separate project, and expectations for Slony-I
should not be based on hopes for future projects.)

<sect1><title> Why doesn't Slony-I do automatic fail-over/promotion? 
</title>

<para>This is the job of network monitoring software, not Slony.
Every site's configuration and fail-over path is different.  For
example, keep-alive monitoring with redundant NIC's and intelligent HA
switches that guarantee race-condition-free takeover of a network
address and disconnecting the "failed" node vary in every network
setup, vendor choice, hardware/software combination.  This is clearly
the realm of network management software and not Slony-I.

<para>Let Slony-I do what it does best: provide database replication.

<sect1><title/ Current Limitations/

<para>Slony-I does not automatically propagate schema changes, nor
does it have any ability to replicate large objects.  There is a
single common reason for these limitations, namely that Slony-I
operates using triggers, and neither schema changes nor large object
operations can raise triggers suitable to tell Slony-I when those
kinds of changes take place.

<para>There is a capability for Slony-I to propagate DDL changes if
you submit them as scripts via the slonik <command/EXECUTE SCRIPT/
operation.  That is not "automatic;" you have to construct an SQL DDL
script and submit it.

<para>If you have those sorts of requirements, it may be worth
exploring the use of PostgreSQL 8.0 PITR (Point In Time Recovery),
where WAL logs are replicated to remote nodes.  Unfortunately, that
has two attendant limitations:

<itemizedlist>
	<listitem><para> PITR replicates <emphasis/all/ changes in <emphasis/all/ databases; you cannot exclude data that isn't relevant;
	<listitem><para> A PITR replica remains dormant until you apply logs and start up the database.  You cannot use the database and apply updates simultaneously.  It is like having a <quote/standby server/ which cannot be used without it ceasing to be <quote/standby./

</itemizedlist>

<para>There are a number of distinct models for database replication; it is impossible for one replication system to be all things to all people.

<!-- Keep this comment at the end of the file
Local variables:
mode:sgml
sgml-omittag:nil
sgml-shorttag:t
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:1
sgml-indent-data:t
sgml-parent-document:slony.sgml
sgml-default-dtd-file:"./reference.ced"
sgml-exposed-tags:nil
sgml-local-catalogs:("/usr/lib/sgml/catalog")
sgml-local-ecat-files:nil
End:
-->