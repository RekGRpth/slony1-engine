<!--  -->
<sect1 id="events">
<title>Events & Confirmations</title>
<para>
&slony1; transfers configuration changes and application data through
events.   Events in &slony1; have an origin, a type and some parameters.   
When an event is created it is inserted
into the event queue (the <xref linkend="table.sl-event"> table) on the node the event
originates on.  The remoteListener threads of slon processes then pick up
that event (by quering the sl_event table) and passing the event to the
slons remoteWorker thread for processing.
</para>

<para>
An event is uniquely identified by taking both the node id of the node the
event originates on and the event sequence number for that node.
For example (1,5000001) identifies event 5000001 originating from node 1.
While (3,5000001) identifies a different event that originated on a 
different node.
</para>

<sect2>
<title>SYNC Events</title>
<para>
SYNC events are used to transfer application data for one node to the next.
When data in a replicated table changes a trigger fires that records information
about the change in the <xref linkend="table.sl-log-1"> or <xref linkend="table.sl-log-2"> tables.  The localListener thread
in the slon processes will then periodically generate a SYNC event.  When the
SYNC event is created &slony1; will find the heighest log_seqid assigned so far
along with a list of log_seqid's that were assigned to transactions that
have not yet been committed.  This information is stored as part of the SYNC 
event.
</para>


<para>
When a slon processes remoteWorker processes a SYNC it will query the rows
from sl_log_1 and sl_log2 that are covered by the SYNC (log_seqid rows
that had been committed at the time the SYNC was generated).  The modifications
described by these rows are then made on the subscriber.
</para>

</sect2>


<sect2>
<title>Event Confirmations</title>
<para>
When an event is processed by the slon for a remote node a CONFIRM message
is generated by inserting a row into the sl_confirm table.  This row indicates
that a particular event was confirmed by a particular receiver node.
Confirmation messages are the transfered back to all other nodes in the cluster.
</para>
</sect2>

<sect2>
<title>Event cleanup</title>
<para>
The slon cleanupThread will periodically run the
<xref linkend="function.cleanupevent-p-interval-interval"> database
function that will delete all but the most recently confirmed event for 
each origin/receiver pair (because if an event has been confirmed by a 
receiver then all older events from that origin have also been confirmed
by the receiver).   Then the function will delete any SYNC events that are
older than the oldest row left in sl_confirm (for each origin). The data 
for these deleted events will also be removed from the sl_log_1 and sl_log_2
tables.
</para>
<para>
When &slony1; is first enabled it will log the data to replicate to the
sl_log_1 table.  After a while it will stop logging to sl_log_1 and switch
to logging on sl_log_2.  When all of rows in  in sl_log_1 have been replicated
&slony1; will TRUNCATE the sl_log_1 table (to clear out the dead tuples),
stop logging to sl_log_2 and switch back to logging to the newly truncated 
sl_log_1 table.  This process will be periodically repeated as &slony1; runs.
</para>

</sect2>

<sect2>
<title>Slonik and Event Confirmations</title>
<para>

&lslonik; can submit configuraiton commands to different event nodes
which is controlled by the parameters of the command.  If two commands
are submitted to different nodes it might be important that they are
processed by other nodes in a consistent order.  The &lslonik; <xref linkend="stmtwaitevent">
command can be used to accomplish this but as of &slony1; 2.1 this is
done automatically by &lslonik; in the following circumstances.
</para>
<orderedlist>
<listitem><para>Before slonik submits an event to a node
it will wait until that the node has confirmed the last configuration event
from the previous event node.</para></listitem>

<listitem><para>Before slonik submits a <xref linkend="stmtsubscribeset">
command it will make sure that the provider node has confirmed all
configuration events from any other node.</para></listitem>

<listitem><para>Before &lslonik; submits a <xref linkend="stmtdropNode"> event
it will make sure that all nodes in the cluster (other than the one
being dropped) have caught up with all other nodes</para></listitem>

<listitem><para>Before slonik submits a <xref linkend="stmtcloneprepare">
it will make sure that the node being cloned is caught up with all other
nodes in the cluster.</para></listitem>

<listitem><para>Before slonik submits a <xref linkend="stmtcreateset"> command
it will make sure that any <xref linkend="stmtdropset"> commands have been confirmed by
all nodes.</para></listitem>

</orderedlist>
<para>

When &lslonik starts it will contact all nodes that it has admin conninfo
information for to find the last non-SYNC event from each node.  Submitting
commands from multiple &lslonik; instances at the same time will confuse &lslonik;
and is not recommended.   If &lslonik is waiting for an event confirmation
it will print a message every 10 seconds saying which events are
outstanding.   Any commands that might require slonik to wait for event
confirmations can not be executed inside of a "try" block (since the
<xref linkend="stmtwaitevent"> command can not be used inside of a "try" block.
</para>

<para>
Automatic waiting for confirmations can be disabled in &lslonik; by running
&lslonik; with the -w option.</para>

<para>


</sect2>



</sect1>
