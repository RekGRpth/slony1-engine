<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Replicating Your First Database</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:cbbrowne@gmail.com"><LINK
REL="HOME"
TITLE="Slony-I 1.1 Administration"
HREF="slony.html"><LINK
REL="UP"
HREF="slonyadmin.html"><LINK
REL="PREVIOUS"
TITLE="Database Schema Changes (DDL)"
HREF="ddlchanges.html"><LINK
REL="NEXT"
TITLE=" More Slony-I Help "
HREF="help.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stdstyle.css"><META
HTTP-EQUIV="Content-Type"></HEAD
><BODY
CLASS="SECT1"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>Slony-I 1.1 Administration</TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="ddlchanges.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
></TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
><A
HREF="help.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="FIRSTDB"
>12. Replicating Your First Database</A
></H1
><P
>In this example, we will be replicating a brand new pgbench
database.  The mechanics of replicating an existing database are
covered here, however we recommend that you learn how
<SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> functions by using a fresh new
non-production database.</P
><P
>The <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> replication engine is
trigger-based, allowing us to replicate databases (or portions
thereof) running under the same postmaster.</P
><P
>This example will show how to replicate the pgbench database
running on localhost (master) to the pgbench slave database also
running on localhost (slave).  We make a couple of assumptions about
your PostgreSQL configuration:

<P
></P
><UL
><LI
><P
> You have <CODE
CLASS="OPTION"
>tcpip_socket=true</CODE
> in your
<TT
CLASS="FILENAME"
>postgresql.conf</TT
> and</P
></LI
><LI
><P
> You have enabled access in your cluster(s) via
<TT
CLASS="FILENAME"
>pg_hba.conf</TT
></P
></LI
></UL
></P
><P
> The <CODE
CLASS="ENVAR"
>REPLICATIONUSER</CODE
> needs to be a PostgreSQL superuser.
This is typically postgres or pgsql.</P
><P
>You should also set the following shell variables:

<P
></P
><UL
><LI
><P
> <CODE
CLASS="ENVAR"
>CLUSTERNAME</CODE
>=slony_example</P
></LI
><LI
><P
> <CODE
CLASS="ENVAR"
>MASTERDBNAME</CODE
>=pgbench</P
></LI
><LI
><P
> <CODE
CLASS="ENVAR"
>SLAVEDBNAME</CODE
>=pgbenchslave</P
></LI
><LI
><P
> <CODE
CLASS="ENVAR"
>MASTERHOST</CODE
>=localhost</P
></LI
><LI
><P
> <CODE
CLASS="ENVAR"
>SLAVEHOST</CODE
>=localhost</P
></LI
><LI
><P
> <CODE
CLASS="ENVAR"
>REPLICATIONUSER</CODE
>=pgsql</P
></LI
><LI
><P
> <CODE
CLASS="ENVAR"
>PGBENCHUSER</CODE
>=pgbench</P
></LI
></UL
></P
><P
>Here are a couple of examples for setting variables in common shells:

<P
></P
><UL
><LI
><P
> bash, sh, ksh
	<TT
CLASS="COMMAND"
>export CLUSTERNAME=slony_example</TT
></P
></LI
><LI
><P
> (t)csh:
	<TT
CLASS="COMMAND"
>setenv CLUSTERNAME slony_example</TT
></P
></LI
></UL
>&#13;</P
><P
><DIV
CLASS="WARNING"
><P
></P
><TABLE
CLASS="WARNING"
WIDTH="100%"
BORDER="0"
><TR
><TD
WIDTH="25"
ALIGN="CENTER"
VALIGN="TOP"
><IMG
SRC="./images/warning.gif"
HSPACE="5"
ALT="Warning"></TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
><P
> If you're changing these variables to use
different hosts for <CODE
CLASS="ENVAR"
>MASTERHOST</CODE
> and <CODE
CLASS="ENVAR"
>SLAVEHOST</CODE
>, be sure
<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>not</I
></SPAN
> to use localhost for either of them.  This will result
in an error similar to the following:&#13;</P
><P
><TT
CLASS="COMMAND"
>ERROR  remoteListenThread_1: db_getLocalNodeId() returned 2 - wrong database?&#13;</TT
></P
></TD
></TR
></TABLE
></DIV
></P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN1161"
>12.1. Creating the pgbenchuser</A
></H2
><P
><TT
CLASS="COMMAND"
>createuser -A -D $PGBENCHUSER</TT
>&#13;</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN1165"
>12.2. Preparing the databases</A
></H2
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    createdb -O $PGBENCHUSER -h $MASTERHOST $MASTERDBNAME
    createdb -O $PGBENCHUSER -h $SLAVEHOST $SLAVEDBNAME
    pgbench -i -s 1 -U $PGBENCHUSER -h $MASTERHOST $MASTERDBNAME</PRE
></TD
></TR
></TABLE
><P
>Because <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> depends on the databases having the pl/pgSQL procedural
language installed, we better install it now.  It is possible that you have
installed pl/pgSQL into the template1 database in which case you can skip this
step because it's already installed into the $MASTERDBNAME.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    createlang plpgsql -h $MASTERHOST $MASTERDBNAME</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
><SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> does not yet automatically copy table definitions from a
master when a slave subscribes to it, so we need to import this data.
We do this with <B
CLASS="APPLICATION"
>pg_dump</B
>.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    pg_dump -s -U $REPLICATIONUSER -h $MASTERHOST $MASTERDBNAME | psql -U $REPLICATIONUSER -h $SLAVEHOST $SLAVEDBNAME</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>To illustrate how <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> allows for on the fly
replication subscription, let's start up <B
CLASS="APPLICATION"
>pgbench</B
>.  If
you run the <B
CLASS="APPLICATION"
>pgbench</B
> application in the foreground of a
separate terminal window, you can stop and restart it with different
parameters at any time.  You'll need to re-export the variables again
so they are available in this session as well.&#13;</P
><P
>The typical command to run <B
CLASS="APPLICATION"
>pgbench</B
> would look like:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    pgbench -s 1 -c 5 -t 1000 -U $PGBENCHUSER -h $MASTERHOST $MASTERDBNAME</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>This will run <B
CLASS="APPLICATION"
>pgbench</B
> with 5 concurrent clients
each processing 1000 transactions against the <B
CLASS="APPLICATION"
>pgbench</B
>
database running on localhost as the pgbench user.&#13;</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN1185"
>12.3. Configuring the Database for Replication.</A
></H2
><P
>Creating the configuration tables, stored procedures, triggers
and configuration is all done through the <A
HREF="app-slonik.html#SLONIK"
><B
CLASS="APPLICATION"
>slonik</B
> </A
> tool.  It is a specialized scripting aid
that mostly calls stored procedures in the master/slave (node)
databases.  The script to create the initial configuration for the
simple master-slave setup of our pgbench database looks like this:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    #!/bin/sh
    
    slonik &#60;&#60;_EOF_
    	#--
    	 # define the namespace the replication system uses in our example it is
    	 # slony_example
    	#--
    	cluster name = $CLUSTERNAME;
    
    	#--
    	 # admin conninfo's are used by slonik to connect to the nodes one for each
    	 # node on each side of the cluster, the syntax is that of PQconnectdb in
    	 # the C-API
    	# --
    	node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER';
    	node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER';
    
    	#--
    	 # init the first node.  Its id MUST be 1.  This creates the schema
    	 # _$CLUSTERNAME containing all replication system specific database
    	 # objects.
    
    	#--
    	init cluster ( id=1, comment = 'Master Node');
     
    	#--
    	 # Because the history table does not have a primary key or other unique
    	 # constraint that could be used to identify a row, we need to add one.
    	 # The following command adds a bigint column named
    	 # _Slony-I_$CLUSTERNAME_rowID to the table.  It will have a default value
    	 # of nextval('_$CLUSTERNAME.s1_rowid_seq'), and have UNIQUE and NOT NULL
    	 # constraints applied.  All existing rows will be initialized with a
    	 # number
    	#--
    	table add key (node id = 1, fully qualified name = 'public.history');
    
    	#--
    	 # Slony-I organizes tables into sets.  The smallest unit a node can
    	 # subscribe is a set.  The following commands create one set containing
    	 # all 4 pgbench tables.  The master or origin of the set is node 1.
    	#--
    	create set (id=1, origin=1, comment='All pgbench tables');
    	set add table (set id=1, origin=1, id=1, fully qualified name = 'public.accounts', comment='accounts table');
    	set add table (set id=1, origin=1, id=2, fully qualified name = 'public.branches', comment='branches table');
    	set add table (set id=1, origin=1, id=3, fully qualified name = 'public.tellers', comment='tellers table');
    	set add table (set id=1, origin=1, id=4, fully qualified name = 'public.history', comment='history table', key = serial);
    
    	#--
    	 # Create the second node (the slave) tell the 2 nodes how to connect to
    	 # each other and how they should listen for events.
    	#--
    
    	store node (id=2, comment = 'Slave node');
    	store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER');
    	store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER');
    	store listen (origin=1, provider = 1, receiver =2);
    	store listen (origin=2, provider = 2, receiver =1);
    _EOF_</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>Is the <B
CLASS="APPLICATION"
>pgbench</B
> still running?  If not start it
again.&#13;</P
><P
>At this point we have 2 databases that are fully prepared.  One
is the master database in which <B
CLASS="APPLICATION"
>pgbench</B
> is busy
accessing and changing rows.  It's now time to start the replication
daemons.&#13;</P
><P
>On $MASTERHOST the command to start the replication engine is

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    slon $CLUSTERNAME "dbname=$MASTERDBNAME user=$REPLICATIONUSER host=$MASTERHOST"</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>Likewise we start the replication system on node 2 (the slave)

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    slon $CLUSTERNAME "dbname=$SLAVEDBNAME user=$REPLICATIONUSER host=$SLAVEHOST"</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>Even though we have the <B
CLASS="APPLICATION"
><A
HREF="app-slon.html#SLON"
> slon</A
></B
> running on both the master and slave, and they
are both spitting out diagnostics and other messages, we aren't
replicating any data yet.  The notices you are seeing is the
synchronization of cluster configurations between the 2
<B
CLASS="APPLICATION"
><A
HREF="app-slon.html#SLON"
> slon </A
></B
>
processes.&#13;</P
><P
>To start replicating the 4 pgbench tables (set 1) from the
master (node id 1) the the slave (node id 2), execute the following
script.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    #!/bin/sh
    slonik &#60;&#60;_EOF_
    	 # ----
    	 # This defines which namespace the replication system uses
    	 # ----
    	 cluster name = $CLUSTERNAME;
    
    	 # ----
    	 # Admin conninfo's are used by the slonik program to connect
    	 # to the node databases.  So these are the PQconnectdb arguments
    	 # that connect from the administrators workstation (where
    	 # slonik is executed).
    	 # ----
    	 node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER';
    	 node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER';
    
    	 # ----
    	 # Node 2 subscribes set 1
    	 # ----
    	 subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);
    _EOF_</PRE
></TD
></TR
></TABLE
>&#13;</P
><P
>Any second now, the replication daemon on $SLAVEHOST will start
to copy the current content of all 4 replicated tables.  While doing
so, of course, the pgbench application will continue to modify the
database.  When the copy process is finished, the replication daemon
on <CODE
CLASS="ENVAR"
>$SLAVEHOST</CODE
> will start to catch up by applying the
accumulated replication log.  It will do this in little steps, 10
seconds worth of application work at a time.  Depending on the
performance of the two systems involved, the sizing of the two
databases, the actual transaction load and how well the two databases
are tuned and maintained, this catchup process can be a matter of
minutes, hours, or eons.</P
><P
>You have now successfully set up your first basic master/slave
replication system, and the 2 databases should, once the slave has
caught up, contain identical data.  That's the theory, at least.  In
practice, it's good to build confidence by verifying that the datasets
are in fact the same.</P
><P
>The following script will create ordered dumps of the 2
databases and compare them.  Make sure that <B
CLASS="APPLICATION"
>pgbench</B
> has
completed its testing, and that your slon sessions have caught up.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    #!/bin/sh
    echo -n "**** comparing sample1 ... "
    psql -U $REPLICATIONUSER -h $MASTERHOST $MASTERDBNAME &#62;dump.tmp.1.$$ &#60;&#60;_EOF_
    	 select 'accounts:'::text, aid, bid, abalance, filler
    		  from accounts order by aid;
    	 select 'branches:'::text, bid, bbalance, filler
    		  from branches order by bid;
    	 select 'tellers:'::text, tid, bid, tbalance, filler
    		  from tellers order by tid;
    	 select 'history:'::text, tid, bid, aid, delta, mtime, filler,
    		  "_Slony-I_${CLUSTERNAME}_rowID"
    		  from history order by "_Slony-I_${CLUSTERNAME}_rowID";
    _EOF_
    psql -U $REPLICATIONUSER -h $SLAVEHOST $SLAVEDBNAME &#62;dump.tmp.2.$$ &#60;&#60;_EOF_
    	 select 'accounts:'::text, aid, bid, abalance, filler
    		  from accounts order by aid;
    	 select 'branches:'::text, bid, bbalance, filler
    		  from branches order by bid;
    	 select 'tellers:'::text, tid, bid, tbalance, filler
    		  from tellers order by tid;
    	 select 'history:'::text, tid, bid, aid, delta, mtime, filler,
    		  "_Slony-I_${CLUSTERNAME}_rowID"
    		  from history order by "_Slony-I_${CLUSTERNAME}_rowID";
    _EOF_
    
    if diff dump.tmp.1.$$ dump.tmp.2.$$ &#62;$CLUSTERNAME.diff ; then
    	 echo "success - databases are equal."
    	 rm dump.tmp.?.$$
    	 rm $CLUSTERNAME.diff
    else
    	 echo "FAILED - see $CLUSTERNAME.diff for database differences"
    fi</PRE
></TD
></TR
></TABLE
></P
><P
>Note that there is somewhat more sophisticated documentation of
the process in the <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> source code tree
in a file called
<TT
CLASS="FILENAME"
>slony-I-basic-mstr-slv.txt</TT
>.</P
><P
>If this script returns <TT
CLASS="COMMAND"
>FAILED</TT
> please contact the
developers at <A
HREF="http://slony.info/"
TARGET="_top"
>http://slony.info/</A
></P
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="ddlchanges.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="slony.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="help.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Database Schema Changes (DDL)</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="slonyadmin.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>More Slony-I Help</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>