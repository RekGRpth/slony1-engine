<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Doing switchover and failover with Slony-I</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:cbbrowne@gmail.com"><LINK
REL="HOME"
TITLE="Slony-I 1.1 Administration"
HREF="slony.html"><LINK
REL="UP"
HREF="slonyadmin.html"><LINK
REL="PREVIOUS"
TITLE="Reshaping a Cluster"
HREF="reshape.html"><LINK
REL="NEXT"
TITLE=" Slony Listen Paths"
HREF="listenpaths.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stdstyle.css"><META
HTTP-EQUIV="Content-Type"></HEAD
><BODY
CLASS="SECT1"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>Slony-I 1.1 Administration</TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="reshape.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
></TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
><A
HREF="listenpaths.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="FAILOVER"
>7. Doing switchover and failover with Slony-I</A
></H1
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN810"
>7.1. Foreword</A
></H2
><P
> <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> is an asynchronous
replication system.  Because of that, it is almost certain that at the
moment the current origin of a set fails, the final transactions
committed at the origin will have not yet propagated to the
subscribers.  Systems are particularly likely to fail under heavy
load; that is one of the corollaries of Murphy's Law.  Therefore the
principal goal is to <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>prevent</I
></SPAN
> the main server from
failing.  The best way to do that is frequent maintenance.</P
><P
> Opening the case of a running server is not exactly what we
should consider a <SPAN
CLASS="QUOTE"
>"professional"</SPAN
> way to do system
maintenance.  And interestingly, those users who found it valuable to
use replication for backup and failover purposes are the very ones
that have the lowest tolerance for terms like <SPAN
CLASS="QUOTE"
>"system
downtime."</SPAN
> To help support these requirements, Slony-I has not
only failover capabilities, but features for controlled origin
transfer.</P
><P
> It is assumed in this document that the reader is familiar with
the <A
HREF="app-slonik.html#SLONIK"
> <B
CLASS="APPLICATION"
>slonik</B
> </A
>
utility and knows at least how to set up a simple 2 node replication
system with <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
>.</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN822"
>7.2. Controlled Switchover</A
></H2
><P
> We assume a current <SPAN
CLASS="QUOTE"
>"origin"</SPAN
> as node1 with one
<SPAN
CLASS="QUOTE"
>"subscriber"</SPAN
> as node2 (<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>e.g.</I
></SPAN
> -
slave).  A web application on a third server is accessing the database
on node1.  Both databases are up and running and replication is more
or less in sync.

<P
></P
><UL
><LI
><P
> At the time of this writing switchover to another
server requires the application to reconnect to the database.  So in
order to avoid any complications, we simply shut down the web server.
Users who use <B
CLASS="APPLICATION"
>pg_pool</B
> for the applications database
connections merely have to shut down the pool.</P
></LI
><LI
><P
> A small slonik script executes the following commands:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    lock set (id = 1, origin = 1);
    wait for event (origin = 1, confirmed = 2);
    move set (id = 1, old origin = 1, new origin = 2);
    wait for event (origin = 1, confirmed = 2);</PRE
></TD
></TR
></TABLE
></P
><P
> After these commands, the origin (master role) of data set 1
has been transferred to node2.  And it is not simply transferred; it
is done in a fashion such that node1 becomes a fully synchronized
subscriber, actively replicating the set.  So the two nodes have
switched roles completely.</P
></LI
><LI
><P
> After reconfiguring the web application (or
<B
CLASS="APPLICATION"
>pgpool</B
>) to connect to the database on node2, the web
server is restarted and resumes normal operation.</P
><P
> Done in one shell script, that does the application shutdown,
<B
CLASS="APPLICATION"
>slonik</B
>, move config files and startup all together, this
entire procedure is likely to take less than 10 seconds.</P
></LI
></UL
></P
><P
> You may now simply shutdown the server hosting node1 and do
whatever is required to maintain the server.  When <B
CLASS="APPLICATION"
><A
HREF="app-slon.html#SLON"
> slon </A
></B
> node1 is restarted later,
it will start replicating again, and soon catch up.  At this point the
procedure to switch origins is executed again to restore the original
configuration.</P
><P
> This is the preferred way to handle things; it runs quickly,
under control of the administrators, and there is no need for there to
be any loss of data.</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN845"
>7.3. Failover</A
></H2
><P
> If some more serious problem occurs on the <SPAN
CLASS="QUOTE"
>"origin"</SPAN
>
server, it may be necessary to failover to a backup server.  This is a
highly undesirable circumstance, as transactions <SPAN
CLASS="QUOTE"
>"committed"</SPAN
> on
the origin, but not applied to the subscribers, will be lost.  You may
have reported these transactions as <SPAN
CLASS="QUOTE"
>"successful"</SPAN
> to outside
users.  As a result, failover should be considered a <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>last
resort</I
></SPAN
>.  If the <SPAN
CLASS="QUOTE"
>"injured"</SPAN
> origin server can be brought up to
the point where it can limp along long enough to do a controlled
switchover, that is <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>greatly</I
></SPAN
> preferable.</P
><P
> Slony does not provide any automatic detection for failed
systems.  Abandoning committed transactions is a business decision
that cannot be made by a database.  If someone wants to put the
commands below into a script executed automatically from the network
monitoring system, well ... it's <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>your</I
></SPAN
> data, and it's <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>your</I
></SPAN
> failover policy.

<P
></P
><UL
><LI
><P
> The <A
HREF="app-slonik.html#SLONIK"
> <B
CLASS="APPLICATION"
>slonik</B
> </A
> command

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    failover (id = 1, backup node = 2);</PRE
></TD
></TR
></TABLE
></P
><P
> causes node2 to assume the ownership (origin) of all sets that
have node1 as their current origin.  If there should happen to be
additional nodes in the <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> cluster, all direct
subscribers of node1 are instructed that this is happening.
<B
CLASS="APPLICATION"
>Slonik</B
> will also query all direct subscribers in order
to determine out which node has the highest replication status
(<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>e.g.</I
></SPAN
> - the latest committed transaction) for each set, and
the configuration will be changed in a way that node2 first applies
those final before actually allowing write access to the tables.</P
><P
> In addition, all nodes that subscribed directly to node1 will
now use node2 as data provider for the set.  This means that after the
failover command succeeded, no node in the entire replication setup
will receive anything from node1 any more.</P
></LI
><LI
><P
> Reconfigure and restart the application (or <B
CLASS="APPLICATION"
>pgpool</B
>)
to cause it to reconnect to node2.</P
></LI
><LI
><P
> After the failover is complete and node2 accepts
write operations against the tables, remove all remnants of node1's
configuration information with the <A
HREF="app-slonik.html#SLONIK"
> <B
CLASS="APPLICATION"
>slonik</B
> </A
> command

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><PRE
CLASS="PROGRAMLISTING"
>    drop node (id = 1, event node = 2);</PRE
></TD
></TR
></TABLE
></P
></LI
></UL
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN876"
>7.4. After Failover, Reconfiguring node1</A
></H2
><P
> After the above failover, the data stored on node1 is
considered out of sync with the rest of the nodes, and must be treated
as corrupt.  Therefore, the only way to get node1 back and transfer
the origin role back to it is to rebuild it from scratch as a
subscriber, let it catch up, and then follow the switchover
procedure.</P
><P
> If the database is very large, it may take many hours to
recover node1 as a functioning <SPAN
CLASS="PRODUCTNAME"
>Slony-I</SPAN
> node; that is
another reason to consider failover as an undesirable <SPAN
CLASS="QUOTE"
>"final
resort."</SPAN
></P
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="reshape.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="slony.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="listenpaths.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Reshaping a Cluster</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="slonyadmin.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>Slony Listen Paths</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>