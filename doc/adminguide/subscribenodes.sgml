<sect1 id="subscribenodes"> <title/ Subscribing Nodes/

<para>Before you subscribe a node to a set, be sure that you have
<application/slon/s running for both the master and the new
subscribing node. If you don't have slons running, nothing will
happen, and you'll beat your head against a wall trying to figure out
what is going on.

<para>Subscribing a node to a set is done by issuing the slonik
command <command/subscribe set/. It may seem tempting to try to
subscribe several nodes to a set within a single try block like this:

<programlisting>
try {
  echo 'Subscribing sets';
  subscribe set (id = 1, provider=1, receiver=2, forward=yes);
  subscribe set (id = 1, provider=1, receiver=3, forward=yes);
  subscribe set (id = 1, provider=1, receiver=4, forward=yes);
} on error {
  echo 'Could not subscribe the sets!';
  exit -1;
}
</programlisting>


<para> You are just asking for trouble if you try to subscribe sets in
that fashion. The proper procedure is to subscribe one node at a time,
and to check the logs and databases before you move onto subscribing
the next node to the set. It is also worth noting that the
<quote/success/ within the above slonik try block does not imply that
nodes 2, 3, and 4 have all been successfully subscribed. It merely
indicates that the slonik commands were successfully received by the
<application/slon/ running on the master node.

<para>A typical sort of problem that will arise is that a cascaded
subscriber is looking for a provider that is not ready yet.  In that
failure case, that subscriber node will <emphasis/never/ pick up the
subscriber.  It will get <quote/stuck/ waiting for a past event to
take place.  The other nodes will be convinced that it is successfully
subscribed (because no error report ever made it back to them); a
request to unsubscribe the node will be <quote/blocked/ because the
node is stuck on the attempt to subscribe it.

<para>When you subscribe a node to a set, you should see something
like this in your slony logs for the master node:

<screen>
DEBUG2 remoteWorkerThread_3: Received event 3,1059 SUBSCRIBE_SET
</screen>

<para>You should also start seeing log entries like this in the slony logs for the subscribing node:

<screen>
DEBUG2 remoteWorkerThread_1: copy table public.my_table
</screen>

<para>It may take some time for larger tables to be copied from the
master node to the new subscriber. If you check the pg_stat_activity
table on the master node, you should see a query that is copying the
table to stdout.

<para>The table <envar/sl_subscribe/ on both the master, and the new
subscriber should contain entries for the new subscription:

<screen>
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
      1  |            1 |            2 |           t |         t
</screen>

<para>A final test is to insert a row into one of the replicated
tables on the master node, and verify that the row is copied to the
new subscriber.

<!-- Keep this comment at the end of the file
Local variables:
mode:sgml
sgml-omittag:nil
sgml-shorttag:t
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:1
sgml-indent-data:t
sgml-parent-document:nil
sgml-default-dtd-file:"./reference.ced"
sgml-exposed-tags:nil
sgml-local-catalogs:("/usr/lib/sgml/catalog")
sgml-local-ecat-files:nil
End:
-->

