<article id="subscribenodes"> <title/ Subscribing Nodes/

<para>Before you subscribe a node to a set, be sure that you have slons running for both the master and the new subscribing node. If you don't have slons running, nothing will happen, and you'll beat your head against a wall trying to figure out what's going on.

<para>Subscribing a node to a set is done by issuing the slonik command "subscribe set". It may seem tempting to try to subscribe several nodes to a set within the same try block like this:

<para> <command>
try {
		  echo 'Subscribing sets';
		  subscribe set (id = 1, provider=1, receiver=2, forward=yes);
		  subscribe set (id = 1, provider=1, receiver=3, forward=yes);
		  subscribe set (id = 1, provider=1, receiver=4, forward=yes);
} on error {
		  echo 'Could not subscribe the sets!';
		  exit -1;
}
</command>

<para> You are just asking for trouble if you try to subscribe sets like that. The proper procedure is to subscribe one node at a time, and to check the logs and databases before you move onto subscribing the next node to the set. It is also worth noting that success within the above slonik try block does not imply that nodes 2, 3, and 4 have all been successfully subscribed. It merely guarantees that the slonik commands were received by the slon running on the master node.

<para>A typical sort of problem that will arise is that a cascaded
subscriber is looking for a provider that is not ready yet.  In that
failure case, that subscriber node will <emphasis/never/ pick up the
subscriber.  It will get "stuck" waiting for a past event to take
place.  The other nodes will be convinced that it is successfully
subscribed (because no error report ever made it back to them); a
request to unsubscribe the node will be "blocked" because the node is
stuck on the attempt to subscribe it.

<para>When you subscribe a node to a set, you should see something like this in your slony logs for the master node:

<para> <command>
DEBUG2 remoteWorkerThread_3: Received event 3,1059 SUBSCRIBE_SET
</command>

<para>You should also start seeing log entries like this in the slony logs for the subscribing node:

<para><command>
DEBUG2 remoteWorkerThread_1: copy table public.my_table
</command>

<para>It may take some time for larger tables to be copied from the master node to the new subscriber. If you check the pg_stat_activity table on the master node, you should see a query that is copying the table to stdout.

<para>The table sl_subscribe on both the master, and the new subscriber should have entries for the new subscription:

<para><Command>
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
	1	  |				1 |				2 | t			  | t
</command>

<para>A final test is to insert a row into a table on the master node, and to see if the row is copied to the new subscriber. 

</article>
<!-- Keep this comment at the end of the file
Local variables:
mode:sgml
sgml-omittag:nil
sgml-shorttag:t
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:1
sgml-indent-data:t
sgml-parent-document:nil
sgml-default-dtd-file:"./reference.ced"
sgml-exposed-tags:nil
sgml-local-catalogs:("/usr/lib/sgml/catalog")
sgml-local-ecat-files:nil
End:
-->

