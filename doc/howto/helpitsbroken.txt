Help!  It's broken!
------------------------------

You're having trouble getting it to work, and are scratching your head
as to what might be wrong.

Here are some idiosyncracies that other people have stumbled over that
might help you to "stumble more quickly."

1.  I looked for the _clustername namespace, and it wasn't there.

If the DSNs are wrong, then slon instances can't connect to the nodes.

This will generally lead to nodes remaining entirely untouched.

Recheck the connection configuration.  By the way, since slon links to
libpq, you could have password information stored in $HOME/.pgpass,
partially filling in right/wrong authentication information there.

2.  Everything in my script _looks_ OK, and some data is getting
pushed around, but not all of it.

Slony logs might look like the following:

DEBUG1 remoteListenThread_1: connected to 'host=host004 dbname=pgbenchrep user=postgres port=5432'
ERROR  remoteListenThread_1: "select ev_origin, ev_seqno, ev_timestamp,        ev_minxid, ev_maxxid, ev_xip,        ev_type,        ev_data1, ev_data2,        ev_data3, ev_data4,        ev_data5, ev_data6,        ev_data7, ev_data8 from "_pgbenchtest".sl_event e where (e.ev_origin = '1' and e.ev_seqno > '1') order by e.ev_origin, e.ev_seqno" - could not receive data from server: Operation now in progress

On AIX and Solaris (and possibly elsewhere), both Slony-I _and
PostgreSQL_ must be compiled with the --enable-thread-safety option.
The above results when PostgreSQL isn't so compiled.

What happens is that the libc (threadsafe) and libpq (non-threadsafe)
use different memory locations for errno, thereby leading to the
request failing.

Problems like this crop up with disadmirable regularity on AIX and
Solaris; it may take something of an "object code audit" to make sure
that ALL of the necessary components have been compiled and linked
with --enable-thread-safety.

For instance, I ran into the problem one that LD_LIBRARY_PATH had been
set, on Solaris, to point to libraries from an old PostgreSQL compile.
That meant that even though the database had been compiled with
--enable-thread-safety, and slon had been compiled against that, slon
was being dynamically linked to the "bad old thread-unsafe version,"
so slon didn't work.  It wasn't clear until I ran "ldd" against slon.

3.  I tried creating a CLUSTER NAME with a "-" in it.  That didn't work.

Slony-I uses the same rules for unquoted identifiers as the PostgreSQL
main parser, so no, you probably shouldn't put a "-" in your
identifier name.

You may be able to defeat this by putting "quotes" around identifier
names, but it's liable to bite you somewhere...

4.  After an immediate stop of postgresql (simulation of system crash)
in pg_catalog.pg_listener a tuple with
relname='_${cluster_name}_Restart' exists. slon doesn't start cause it
thinks another process is serving the cluster on this node.  What can
I do? The tuples can't be dropped from this relation.

Answer:  

Before starting slon, do a 'restart node'. PostgreSQL tries to notify
the listeners and drop those are not answering. Slon then starts
cleanly.

5.  If I run a "ps" command, I, and everyone else, can see passwords
on the command line.

Take the passwords out of the Slony configuration, and put them into
$(HOME)/.pgpass.

6.  When I run the sample setup script I get an error message similar
to:

<stdin>:64: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  LOAD:
could not open file '$libdir/xxid': No such file or directory

Evidently, you haven't got the xxid.so library in the $libdir
directory that the PostgreSQL instance is using.  Note that the Slony
components need to be installed on EACH ONE of the nodes, not just on
the "master."

This may also point to there being some other mismatch between the
PostgreSQL binary instance and the Slony-I instance.  If you compiled
Slony-I yourself, on a machine that may have multiple PostgreSQL
builds "lying around," it's possible that the slon or slonik binaries
are asking to load something that isn't actually in the library
directory for the PostgreSQL database cluster that it's hitting.

Long and short: This points to a need to "audit" what installations of
PostgreSQL and Slony you have in place on the machine(s).
Unfortunately, just about any mismatch will cause things not to link
up quite right.  Look back at #2...

7.  An oddity - no need for Fully Qualified Name for table keys...

set add table (set id = 1, origin = 1, id = 27, full qualified name = 'nspace.some_table', key = 'key_on_whatever', 
    comment = 'Table some_table in namespace nspace with a candidate primary key');

If you have
   key = 'nspace.key_on_whatever'
the request will FAIL.

8.  I'm trying to get a slave subscribed, and get the following
messages in the logs:

DEBUG1 copy_set 1
DEBUG1 remoteWorkerThread_1: connected to provider DB
WARN   remoteWorkerThread_1: transactions earlier than XID 127314958 are still in progress
WARN   remoteWorkerThread_1: data copy for set 1 failed - sleep 60 seconds

Oops.  What I forgot to mention, as well, was that I was trying to add
TWO subscribers, concurrently.

That doesn't work out: Slony-I won't work on the COPY commands
concurrently.  See src/slon/remote_worker.c, function copy_set()

This has the (perhaps unfortunate) implication that you cannot
populate two slaves concurrently.  You have to subscribe one to the
set, and only once it has completed setting up the subscription
(copying table contents and such) can the second subscriber start
setting up the subscription.

It could also be possible for there to be an old outstanding
transaction blocking Slony-I from processing the sync.  You might want
to take a look at pg_locks to see what's up:

sampledb=# select * from pg_locks where transaction is not null order by transaction;
 relation | database | transaction |   pid   |     mode      | granted 
----------+----------+-------------+---------+---------------+---------
          |          |   127314921 | 2605100 | ExclusiveLock | t
          |          |   127326504 | 5660904 | ExclusiveLock | t
(2 rows)

See?  127314921 is indeed older than 127314958, and it's still running.

$ ps -aef | egrep '[2]605100'
postgres 2605100  205018   0 18:53:43  pts/3  3:13 postgres: postgres sampledb localhost COPY 

This happens to be a COPY transaction involved in setting up the
subscription for one of the nodes.  All is well; the system is busy
setting up the first subscriber; it won't start on the second one
until the first one has completed subscribing.

9.  I tried setting up a second replication set, and got the following error:

<stdin>:9: Could not create subscription set 2 for oxrslive!
<stdin>:11: PGRES_FATAL_ERROR select "_oxrslive".setAddTable(2, 1, 'public.replic_test', 'replic_test__Slony-I_oxrslive_rowID_key', 'Table public.replic_test without primary key');  - ERROR:  duplicate key violates unique constraint "sl_table-pkey"
CONTEXT:  PL/pgSQL function "setaddtable_int" line 71 at SQL statement

The table IDs used in SET ADD TABLE are required to be unique ACROSS
ALL SETS.  Thus, you can't restart numbering at 1 for a second set; if
you are numbering them consecutively, a subsequent set has to start
with IDs after where the previous set(s) left off.

10.  I need to drop a table from a replication set, and I can't see
how to do that.

This can be accomplished several ways, not all equally desirable ;-).

a) You could drop the whole replication set, and recreate it with just
the tables that you need.  Alas, that means recopying a whole lot of
data, and kills the usability of the cluster on the rest of the set
while that's happening.

b) Some day, someone will get around to implementing the Slonik
command SET DROP TABLE, which will involve (roughly speaking) the
following set of changes:

 1.  Need to augment the grammar to know about "set drop table"

 2.  Need to have a C function for set_drop_table

 3.  Need a stored procedure, droptable(), which initiates the event

 4.  Need another stored procedure, droptable_int() which does the
      alterTableRestore(), tableDropKey()

 5.  I presume (haven't looked) that slon needs to have a case
      statement added to allow it to propagate the "DROP_TABLE" 
      event.

That will happen when someone gets around to implementing it.  The
likely "first volunteer" is Chris Browne <cbbrowne@acm.org>, but if
somone gets to it first, he won't mind too much.

c) The _essential_ functionality of SET DROP TABLE is the bits in step
4, droptable_int().  You can fiddle this by hand by finding the table
ID for the table you want to get rid of, which you can find in
sl_table, and then run three queries, on each host:

  select _slonyschema.alterTableRestore(40);
  select _slonyschema.tableDropKey(40);
  delete from _slonyschema.sl_table where tab_id = 40;

The schema will obviously depend on how you defined the Slony-I
cluster.  The table ID, in this case, 40, will need to change to the
ID of the table you want to have go away.
